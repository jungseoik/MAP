import gradio as gr
from pathlib import Path
abs_path = Path(__file__).parent
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from sheet_manager.sheet_loader.sheet2df import sheet2df
from sheet_manager.sheet_convert.json2sheet import str2json
# Mock ë°ì´í„° ìƒì„±
def calculate_avg_metrics(df):
    """
    ê° ëª¨ë¸ì˜ ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°
    """
    metrics_data = []
    
    for _, row in df.iterrows():
        model_name = row['Model name']
        
        # PIAê°€ ë¹„ì–´ìˆê±°ë‚˜ ë‹¤ë¥¸ ê°’ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if pd.isna(row['PIA']) or not isinstance(row['PIA'], str):
            print(f"Skipping model {model_name}: Invalid PIA data")
            continue
            
        try:
            metrics = str2json(row['PIA'])
            
            # metricsê°€ Noneì´ê±°ë‚˜ dictê°€ ì•„ë‹Œ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if not metrics or not isinstance(metrics, dict):
                print(f"Skipping model {model_name}: Invalid JSON format")
                continue
                
            # í•„ìš”í•œ ì¹´í…Œê³ ë¦¬ê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
            required_categories = ['falldown', 'violence', 'fire']
            if not all(cat in metrics for cat in required_categories):
                print(f"Skipping model {model_name}: Missing required categories")
                continue
                
            # í•„ìš”í•œ ë©”íŠ¸ë¦­ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
            required_metrics = ['accuracy', 'precision', 'recall', 'specificity', 'f1', 
                              'balanced_accuracy', 'g_mean', 'mcc', 'npv', 'far']
            
            avg_metrics = {}
            for metric in required_metrics:
                try:
                    values = [metrics[cat][metric] for cat in required_categories 
                             if metric in metrics[cat]]
                    if values:  # ê°’ì´ ìˆëŠ” ê²½ìš°ë§Œ í‰ê·  ê³„ì‚°
                        avg_metrics[metric] = sum(values) / len(values)
                    else:
                        avg_metrics[metric] = 0  # ë˜ëŠ” ë‹¤ë¥¸ ê¸°ë³¸ê°’ ì„¤ì •
                except (KeyError, TypeError) as e:
                    print(f"Error calculating {metric} for {model_name}: {str(e)}")
                    avg_metrics[metric] = 0  # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
            
            metrics_data.append({
                'model_name': model_name,
                **avg_metrics
            })
            
        except Exception as e:
            print(f"Error processing model {model_name}: {str(e)}")
            continue
    
    return pd.DataFrame(metrics_data)

def create_performance_chart(df, selected_metrics):
    """
    ëª¨ë¸ë³„ ì„ íƒëœ ì„±ëŠ¥ ì§€í‘œì˜ ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±
    """
    fig = go.Figure()
    
    # ëª¨ë¸ ì´ë¦„ ê¸¸ì´ì— ë”°ë¥¸ ë§ˆì§„ ê³„ì‚°
    max_name_length = max([len(name) for name in df['model_name']])
    left_margin = min(max_name_length * 7, 500)  # ê¸€ì ìˆ˜ì— ë”°ë¼ ë§ˆì§„ ì¡°ì •, ìµœëŒ€ 500
    
    for metric in selected_metrics:
        fig.add_trace(go.Bar(
            name=metric,
            y=df['model_name'],  # yì¶•ì— ëª¨ë¸ ì´ë¦„
            x=df[metric],        # xì¶•ì— ì„±ëŠ¥ ì§€í‘œ ê°’
            text=[f'{val:.3f}' for val in df[metric]],
            textposition='auto',
            orientation='h'      # ìˆ˜í‰ ë°©í–¥ ë§‰ëŒ€
        ))
    
    fig.update_layout(
        title='Model Performance Comparison',
        yaxis_title='Model Name',
        xaxis_title='Performance',
        barmode='group',
        height=max(400, len(df) * 40),  # ëª¨ë¸ ìˆ˜ì— ë”°ë¼ ë†’ì´ ì¡°ì •
        margin=dict(l=left_margin, r=50, t=50, b=50),  # ì™¼ìª½ ë§ˆì§„ ë™ì  ì¡°ì •
        showlegend=True,
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        ),
        yaxis={'categoryorder': 'total ascending'}  # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬
    )
    
    # yì¶• ë ˆì´ë¸” ìŠ¤íƒ€ì¼ ì¡°ì •
    fig.update_yaxes(tickfont=dict(size=10))  # ê¸€ì í¬ê¸° ì¡°ì •
    
    return fig
def create_confusion_matrix(metrics_data, selected_category):
    """í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ìƒì„±"""
    # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ í˜¼ë™ í–‰ë ¬ ë°ì´í„°
    tp = metrics_data[selected_category]['tp']
    tn = metrics_data[selected_category]['tn']
    fp = metrics_data[selected_category]['fp']
    fn = metrics_data[selected_category]['fn']
    
    # í˜¼ë™ í–‰ë ¬ ë°ì´í„°
    z = [[tn, fp], [fn, tp]]
    x = ['Negative', 'Positive']
    y = ['Negative', 'Positive']
    
    # íˆíŠ¸ë§µ ìƒì„±
    fig = go.Figure(data=go.Heatmap(
        z=z,
        x=x,
        y=y,
        colorscale=[[0, '#f7fbff'], [1, '#08306b']],
        showscale=False,
        text=[[str(val) for val in row] for row in z],
        texttemplate="%{text}",
        textfont={"color": "black", "size": 16},  # ê¸€ì ìƒ‰ìƒì„ ê²€ì •ìƒ‰ìœ¼ë¡œ ê³ ì •
    ))
    
    # ë ˆì´ì•„ì›ƒ ì—…ë°ì´íŠ¸
    fig.update_layout(
        title={
            'text': f'Confusion Matrix - {selected_category}',
            'y':0.9,
            'x':0.5,
            'xanchor': 'center',
            'yanchor': 'top'
        },
        xaxis_title='Predicted',
        yaxis_title='Actual',
        width=600,  # ë„ˆë¹„ ì¦ê°€
        height=600,  # ë†’ì´ ì¦ê°€
        margin=dict(l=80, r=80, t=100, b=80),  # ì—¬ë°± ì¡°ì •
        paper_bgcolor='white',
        plot_bgcolor='white',
        font=dict(size=14)  # ì „ì²´ í°íŠ¸ í¬ê¸° ì¡°ì •
    )
    
    # ì¶• ì„¤ì •
    fig.update_xaxes(side="bottom", tickfont=dict(size=14))
    fig.update_yaxes(side="left", tickfont=dict(size=14))

    return fig

def get_metrics_for_model(df, model_name, benchmark_name):
    """íŠ¹ì • ëª¨ë¸ê³¼ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•œ ë©”íŠ¸ë¦­ìŠ¤ ë°ì´í„° ì¶”ì¶œ"""
    row = df[(df['Model name'] == model_name) & (df['Benchmark'] == benchmark_name)]
    if not row.empty:
        metrics = str2json(row['PIA'].iloc[0])
        return metrics
    return None

def metric_visual_tab():
    # ë°ì´í„° ë¡œë“œ
    df = sheet2df(sheet_name="metric")
    avg_metrics_df = calculate_avg_metrics(df)
    
    # ê°€ëŠ¥í•œ ëª¨ë“  ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸
    all_metrics = ['accuracy', 'precision', 'recall', 'specificity', 'f1', 
                  'balanced_accuracy', 'g_mean', 'mcc', 'npv', 'far']

    with gr.Tab("ğŸ“Š Performance Visualization"):
        with gr.Row():
            metrics_multiselect = gr.CheckboxGroup(
                choices=all_metrics,
                value=[],  # ì´ˆê¸° ì„ íƒ ì—†ìŒ
                label="Select Performance Metrics",
                interactive=True
            )
        
        # Performance comparison chart (ì´ˆê¸°ê°’ ì—†ìŒ)
        performance_plot = gr.Plot()
        
        def update_plot(selected_metrics):
            if not selected_metrics:  # ì„ íƒëœ ë©”íŠ¸ë¦­ì´ ì—†ëŠ” ê²½ìš°
                return None
            
            try:
                # accuracy ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                sorted_df = avg_metrics_df.sort_values(by='accuracy', ascending=True)
                return create_performance_chart(sorted_df, selected_metrics)
            except Exception as e:
                print(f"Error in update_plot: {str(e)}")
                return None
        
        # Connect event handler
        metrics_multiselect.change(
            fn=update_plot,
            inputs=[metrics_multiselect],
            outputs=[performance_plot]
        )
        
def create_category_metrics_chart(metrics_data, selected_metrics):
    """
    ì„ íƒëœ ëª¨ë¸ì˜ ê° ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™”
    """
    fig = go.Figure()
    categories = ['falldown', 'violence', 'fire']
    
    for metric in selected_metrics:
        values = []
        for category in categories:
            values.append(metrics_data[category][metric])
        
        fig.add_trace(go.Bar(
            name=metric,
            x=categories,
            y=values,
            text=[f'{val:.3f}' for val in values],
            textposition='auto',
        ))

    fig.update_layout(
        title='Performance Metrics by Category',
        xaxis_title='Category',
        yaxis_title='Score',
        barmode='group',
        height=500,
        showlegend=True,
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )
    
    return fig

def metric_visual_tab():
    # ë°ì´í„° ë¡œë“œ ë° ì²« ë²ˆì§¸ ì‹œê°í™” ë¶€ë¶„
    df = sheet2df(sheet_name="metric")
    avg_metrics_df = calculate_avg_metrics(df)

    # ê°€ëŠ¥í•œ ëª¨ë“  ë©”íŠ¸ë¦­ ë¦¬ìŠ¤íŠ¸
    all_metrics = ['accuracy', 'precision', 'recall', 'specificity', 'f1', 
                    'balanced_accuracy', 'g_mean', 'mcc', 'npv', 'far']

    with gr.Tab("ğŸ“Š Performance Visualization"):
        with gr.Row():
            metrics_multiselect = gr.CheckboxGroup(
                choices=all_metrics,
                value=[],  # ì´ˆê¸° ì„ íƒ ì—†ìŒ
                label="Select Performance Metrics",
                interactive=True
            )
        
        performance_plot = gr.Plot()
        
        def update_plot(selected_metrics):
            if not selected_metrics:
                return None
            try:
                sorted_df = avg_metrics_df.sort_values(by='accuracy', ascending=True)
                return create_performance_chart(sorted_df, selected_metrics)
            except Exception as e:
                print(f"Error in update_plot: {str(e)}")
                return None
        
        metrics_multiselect.change(
            fn=update_plot,
            inputs=[metrics_multiselect],
            outputs=[performance_plot]
        )
        
        # ë‘ ë²ˆì§¸ ì‹œê°í™” ì„¹ì…˜
        gr.Markdown("## Detailed Model Analysis")
        
        with gr.Row():
            # ëª¨ë¸ ì„ íƒ
            model_dropdown = gr.Dropdown(
                choices=sorted(df['Model name'].unique().tolist()),
                label="Select Model",
                interactive=True
            )
            
            # ì»¬ëŸ¼ ì„ íƒ (Model name ì œì™¸)
            column_dropdown = gr.Dropdown(
                choices=[col for col in df.columns if col != 'Model name'],
                label="Select Metric Column",
                interactive=True
            )
            
            # ì¹´í…Œê³ ë¦¬ ì„ íƒ
            category_dropdown = gr.Dropdown(
                choices=['falldown', 'violence', 'fire'],
                label="Select Category",
                interactive=True
            )
        
            # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("") # ë¹ˆ ê³µê°„
            with gr.Column(scale=2):
                confusion_matrix_plot = gr.Plot(container=True)  # container=True ì¶”ê°€
            with gr.Column(scale=1):
                gr.Markdown("") # ë¹ˆ ê³µê°„

        with gr.Column(scale=2):
            # ì„±ëŠ¥ ì§€í‘œ ì„ íƒ
            metrics_select = gr.CheckboxGroup(
                choices=['accuracy', 'precision', 'recall', 'specificity', 'f1', 
                        'balanced_accuracy', 'g_mean', 'mcc', 'npv', 'far'],
                value=['accuracy'],  # ê¸°ë³¸ê°’
                label="Select Metrics to Display",
                interactive=True
            )
            category_metrics_plot = gr.Plot()

        def update_visualizations(model, column, category, selected_metrics):
            if not all([model, column]):  # categoryëŠ” í˜¼ë™í–‰ë ¬ì—ë§Œ í•„ìš”
                return None, None
                
            try:
                # ì„ íƒëœ ëª¨ë¸ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                selected_data = df[df['Model name'] == model][column].iloc[0]
                metrics = str2json(selected_data)
                
                if not metrics:
                    return None, None
                    
                # í˜¼ë™ í–‰ë ¬ (ì™¼ìª½)
                confusion_fig = create_confusion_matrix(metrics, category) if category else None
                
                # ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ì§€í‘œ (ì˜¤ë¥¸ìª½)
                if not selected_metrics:
                    selected_metrics = ['accuracy']
                category_fig = create_category_metrics_chart(metrics, selected_metrics)
                
                return confusion_fig, category_fig
                
            except Exception as e:
                print(f"Error updating visualizations: {str(e)}")
                return None, None
        
        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
        for input_component in [model_dropdown, column_dropdown, category_dropdown, metrics_select]:
            input_component.change(
                fn=update_visualizations,
                inputs=[model_dropdown, column_dropdown, category_dropdown, metrics_select],
                outputs=[confusion_matrix_plot, category_metrics_plot]
            )        
        # def update_confusion_matrix(model, column, category):
        #     if not all([model, column, category]):
        #         return None
                
        #     try:
        #         # ì„ íƒëœ ëª¨ë¸ì˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        #         selected_data = df[df['Model name'] == model][column].iloc[0]
        #         metrics = str2json(selected_data)
                
        #         if metrics and category in metrics:
        #             category_data = metrics[category]
                    
        #             # í˜¼ë™ í–‰ë ¬ ë°ì´í„°
        #             confusion_data = {
        #                 'tp': category_data['tp'],
        #                 'tn': category_data['tn'],
        #                 'fp': category_data['fp'],
        #                 'fn': category_data['fn']
        #             }
                    
        #             # íˆíŠ¸ë§µ ìƒì„±
        #             z = [[confusion_data['tn'], confusion_data['fp']], 
        #                     [confusion_data['fn'], confusion_data['tp']]]
                    
        #             fig = go.Figure(data=go.Heatmap(
        #                 z=z,
        #                 x=['Negative', 'Positive'],
        #                 y=['Negative', 'Positive'],
        #                 text=[[str(val) for val in row] for row in z],
        #                 texttemplate="%{text}",
        #                 textfont={"size": 16},
        #                 colorscale='Blues',
        #                 showscale=False
        #             ))
                    
        #             fig.update_layout(
        #                 title=f'Confusion Matrix - {category}',
        #                 xaxis_title='Predicted',
        #                 yaxis_title='Actual',
        #                 width=500,
        #                 height=500
        #             )
                    
        #             return fig
                    
        #     except Exception as e:
        #         print(f"Error updating confusion matrix: {str(e)}")
        #         return None
        
        # # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
        # for dropdown in [model_dropdown, column_dropdown, category_dropdown]:
        #     dropdown.change(
        #         fn=update_confusion_matrix,
        #         inputs=[model_dropdown, column_dropdown, category_dropdown],
        #         outputs=confusion_matrix_plot
        #     )
       



